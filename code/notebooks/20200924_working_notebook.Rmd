---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

*20200924* 

# Train with DLC

Pilot videos here: `/nfs/ftp/private/birney-res-ftp/upload/medaka/videos/ian_pilot/all`

**NOTE** exclusion of `20190613_1156_icab_kaga_R` because file is corrupted.
**NOTE** exclusion of `20190616_1227_icab_kaga_R_q3` because quadrant is empty.

# Interactive shell on GPU

```{bash, eval = F}
bsub -M 10000 -P gpu -gpu - -Is bash
```


## Install DeepLabCut on local

Following tutorial here: <https://github.com/DeepLabCut/DeepLabCut/blob/master/docs/installation.md>

```{bash}
# move to repo directory
cd ~/Documents/Repositories
# clone repo
git clone https://github.com/AlexEMG/DeepLabCut.git
# move into repo
cd DeepLabCut
# create environment - note creating CPU version
conda env create -f conda-environments/DLC-CPU.yaml
# activate env
conda activate DLC-CPU
# upgrade to version 2.2b8
pip install deeplabcut==2.2b8 # NOTE: this didn't work, but rather I re-created the env having edited the YAML to specify this version
# activate python to start GUI
pythonw
```

Start GUI

Shortcut: `pythonw -m deeplabcut`

```{python}
# import
import deeplabcut
# check version
deeplabcut.__version__
# launch GUI
deeplabcut.launch_dlc()
```

## Import 10 videos for training

Saved here: `data/20201019_training_vids.txt`

* 20190611_1628_icab_hdr_R.avi
* 20190612_0952_icab_icab_R.avi
* 20190613_1024_icab_hni_R.avi
* 20190614_1208_icab_icab_L.avi
* 20190614_1456_icab_ho5_R.avi
* 20190614_1553_icab_hni_L.avi
* 20190615_1051_icab_hdr_L.avi
* 20190615_1518_icab_kaga_L.avi
* 20190616_1017_icab_kaga_R.avi
* 20190616_1622_icab_ho5_L.avi

```{bash}
for i in $( cat data/20201019_training_vids.txt ) ; do
  scp brettell@ebi:/nfs/ftp/private/birney-res-ftp/upload/medaka/videos/ian_pilot/all/$i \
  ~/Documents/Data/20201019_pilot_dlc_training ;
done  
```

## Project admin

* Project name: `~/Documents/pilot_paper`
* 10 videos loaded from `~/Documents/Data/20201019_pilot_dlc_training`
* config.yaml: `~/Documents/pilot_paper-ian_brettell-2020-10-21/config.yaml`
### Extract frames
* Use `uniform` not `kmeans` to extract frames, because the latter takes empty frames and ones with my arm in it. 

### Skeleton

```{r}
# Create vector of all body parts
bparts = c("nose", "head", "left_eye", "right_eye","spine_1", "spine_2", "spine_3", "spine_4", "spine_5", "spine_6", "tail_fin")
# Create pair-wise combinations
bpart_combs = t(combn(bparts, 2))

write.table(bpart_combs, here("pilot_paper", "data", "20201022_body_part_combinations.txt"),
            row.names = F,
            col.names = F,
            quote = F,
            sep = "\t")
# Then manually adjusted file on text editor

# Find:
# ^
# Replace:
# - - 

# Find:
# \t
# Replace:
# \n  - 
```

# Create conda environments for project

```{bash}
cd ~/Documents/Repositories/pilot_paper
# Python env
## Create env
conda create -n pilot_env_py python=3.6 biopython
## Activate env
conda activate pilot_env_py
## List current packages
conda list
## Install packages
conda install -c anaconda opencv
conda install -c anaconda pandas
## Write to file
conda env export -n pilot_env_py -f config/pilot_env_py.yml --from-history

# Snakemake env
mamba create -c conda-forge -c bioconda -n snakemake snakemake
conda activate snakemake
## Send to file
conda env export > config/snakemake.yml

# R env
conda create -n pilot_env_r
conda activate pilot_env_r
## Install packages
conda config --add channels conda-forge
conda config --add channels defaults
conda config --add channels r
conda config --add channels bioconda
conda install r
conda install rstudio
conda install r-tidyverse
## Send to file
conda env export > config/pilot_env_r.yml

```

## Create envs on cluster
```{bash}
# PYTHON ENV
conda env create -f pilot_paper/config/pilot_env_py.yml
# doesn't work beccuse of cross-platform issues between macos and linux
#Solving environment: failed
#ResolvePackageNotFound: 

# Make afresh 
conda create -n pilot_py python=3.6 biopython
conda activate pilot_py

conda install -c anaconda opencv
conda install -c anaconda pandas
# Export to config
conda env export -n pilot_py -f pilot_paper/envs/pilot_py.yml

# DLC ENV

```


# Edit videos with Python

## Edit coordinate file

```{python}
import pandas as pd
# read in
dat = pd.read_csv('/Users/brettell/Documents/Repositories/pilot_paper/data/20201105_crop_coords.csv')
# get head
dat.head()
#
```

## Find codec of video
<https://stackoverflow.com/questions/2869281/how-to-determine-video-codec-of-a-file-with-ffmpeg>
```{bash}
ffprobe -v error -select_streams v:0 -show_entries stream=codec_name \
  -of default=noprint_wrappers=1:nokey=1 /Users/brettell/Documents/Data/20201019_pilot_dlc_training/20190613_1024_icab_hni_R.avi
```


## Crop and save videos

Tutorial: <https://docs.opencv.org/4.5.0/dd/d43/tutorial_py_video_display.html>

```{python}
# Activate environment
conda activate pilot_env_py
# Open IPython
ipython
# Import libraries
import numpy as np
import cv2 as cv
import os 

# Set path of file
path = '/Users/brettell/Documents/Data/20201019_pilot_dlc_training/20190613_1024_icab_hni_R.avi'

# Get file name without extension
file_name = os.path.split(path)[1].split('.')[0].split('_')

# Get metadata 
date = file_name[0]
time = file_name[1]
test_line = file_name[3]
tank_side = file_name[4]

# Read video from file
cap = cv.VideoCapture(path)

# Play video until completed
while cap.isOpened():
    # Capture frame-by-frame
    ret, frame = cap.read()
    # if frame is read correctly ret is True
    if not ret:
        print("Can't receive frame (stream end?). Exiting ...")
        break
    # Flip if tank side is "R"    
    if tank_side == 'R':
        frame = cv.rotate(frame, cv.ROTATE_180)
    # Show image    
    cv.imshow('frame', frame)
    # Press 'esc' to close video
    if cv.waitKey(1) == 27:
        cv.destroyAllWindows()
        cv.waitKey(1)
        break    

cap.release()
        
# Get frame width and height
wid = int(cap.get(cv.CAP_PROP_FRAME_WIDTH))
hei = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))

# Get total frame length
int(cap.get(cv.CAP_PROP_FRAME_COUNT))

# Get frames per second
int(cap.get(cv.CAP_PROP_FPS))

# Get FPS for each video in directory
data_path = '/Users/brettell/Documents/Data/20201019_pilot_dlc_training'
files = os.listdir(data_path)
for file in files:
    # get full path name
    file_full = os.path.join(data_path, file)
    # print 
    if os.path.isfile(file_full):
        cap = cv.VideoCapture(file_full)
        fps = cap.get(cv.CAP_PROP_FPS)
        print(fps)

# Do the same but on the cluster, and get total frame length    
data_path = '/nfs/ftp/private/birney-res-ftp/upload/medaka/videos/ian_pilot/all'
files = os.listdir(data_path)
files.sort() # sort files
for file in files:
    # get full path name
    file_full = os.path.join(data_path, file)
    # print 
    if os.path.isfile(file_full):
        cap = cv.VideoCapture(file_full)
        vid_len = int(cap.get(cv.CAP_PROP_FRAME_COUNT))
        print(vid_len)        
        
# Play select frames of video
start = 10000
end = 10020
for i in range(start,end):
    cap.set(cv.CAP_PROP_POS_FRAMES, i)
    # Capture frame-by-frame
    ret, frame = cap.read()
    # if frame is read correctly ret is True
    if not ret:
        print("Can't receive frame (stream end?). Exiting ...")
        break
    # Flip if tank side is "R"    
    if tank_side == 'R':
        frame = cv.rotate(frame, cv.ROTATE_180)
    # Show image    
    cv.imshow('frame', frame)
    # Press 'esc' to close video
    if cv.waitKey(1) == 27:
        cv.destroyAllWindows()
        cv.waitKey(1)
        break
cv.destroyAllWindows()
cv.waitKey(1)
           
# Show specific frames, and cycle through with spacebar
start = 10000
end = 10020
i = start
while i in range(start,end):
    cap.set(cv.CAP_PROP_POS_FRAMES, i)
    # Capture frame-by-frame
    ret, frame = cap.read()
    # if frame is read correctly ret is True
    if not ret:
        print("Can't receive frame (stream end?). Exiting ...")
        break
    # Flip if tank side is "R"    
    if tank_side == 'R':
        frame = cv.rotate(frame, cv.ROTATE_180)
    # Show image    
    cv.imshow('frame', frame)
    # Wait for spacebar to continue        
    if cv.waitKey(0) == 32:
        # Add to counter
        i += 1
    # Press 'esc' to close video
    elif cv.waitKey(0) == 27:
        cv.destroyAllWindows()
        cv.waitKey(1)
        break    
cv.destroyAllWindows()
cv.waitKey(1)

# Trackbar question:
# <https://stackoverflow.com/questions/21983062/in-python-opencv-is-there-a-way-to-quickly-scroll-through-frames-of-a-video-all>

# Pull out frame to find coordinates
write_path = '/Users/brettell/Documents/Data/20201019_pilot_dlc_training_processed'


## Whole frame
target = 10000

cap.set(cv.CAP_PROP_POS_FRAMES, target)
# Capture frame-by-frame
ret, frame = cap.read()
# if frame is read correctly ret is True
if not ret:
    print("Can't receive frame (stream end?). Exiting ...")
# Flip if tank side is "R"    
if tank_side == 'R':
    frame = cv.rotate(frame, cv.ROTATE_180)
# Get full path name
path_out = os.path.join(write_path, str(target) + '.png')
# Write frame 
cv.imwrite(path_out, frame)

## Save separate quadrants
### Take mid-points and give 10 pixels' leeway
cap.set(cv.CAP_PROP_POS_FRAMES, target)
# Capture frame-by-frame
ret, frame = cap.read()
# if frame is read correctly ret is True
if not ret:
    print("Can't receive frame (stream end?). Exiting ...")
# Flip if tank side is "R"    
if tank_side == 'R':
    frame = cv.rotate(frame, cv.ROTATE_180)
# Pull out quadrants and save
for x in ['q1', 'q2', 'q3', 'q4']:
    if x == 'q1':
        top = 0 
        bottom = round(((hei - 1) / 2) + 10) 
        left = round((wid - 1) / 2)
        right = wid - 1
    if x == 'q2':
        top = 0 
        bottom = round(((hei - 1) / 2) + 10) 
        left = 0
        right = round(((wid - 1) / 2) + 10)
    if x == 'q3':
        top = round(((hei - 1) / 2) + 5) 
        bottom = hei - 1
        left = 0
        right = round(((wid - 1) / 2) + 10)
    if x == 'q4':
        top = round(((hei - 1) / 2) + 5) 
        bottom = hei - 1
        left = round(((wid - 1) / 2) + 5)    
        right = wid  - 1  
    # Set bounding box
    cropped_img = frame[top:bottom, left:right]
    # Get full path name
    path_out = os.path.join(write_path, str(target) + '_' + x + '.png')
    # Write frame 
    cv.imwrite(path_out, cropped_img)
```

## Create new video and save

Troubleshooting help here: `https://stackoverflow.com/questions/26452909/opencv-write-frame-to-file-python`
```{python}
# Import libraries
import numpy as np
import cv2 as cv
import os

# Get key variables

in_file = '/Users/brettell/Documents/Data/20201019_pilot_dlc_training/20190613_1024_icab_hni_R.avi'
start = 2889
end = 22790
quadrant = 'q1'

# Read video from file
cap = cv.VideoCapture(in_file)

# Frame width and height
wid = int(cap.get(cv.CAP_PROP_FRAME_WIDTH))
hei = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))
# Get total frame length
vid_len = int(cap.get(cv.CAP_PROP_FRAME_COUNT))
# Get frames per second
fps = int(cap.get(cv.CAP_PROP_FPS))
# Adapt basename prefix
in_file_pref = os.path.split(in_file)[1].split('.')[0]
# Get file name without extension
meta_list = os.path.split(in_file)[1].split('.')[0].split('_')

# Get metadata 
date = meta_list[0]
time = meta_list[1]
test_line = meta_list[3]
tank_side = meta_list[4]

# Output file
out_dir = '/Users/brettell/Documents/Data/20201019_pilot_dlc_training_processed'
out_file = os.path.join(out_dir, in_file_pref + '_' + quadrant + '.mp4')

# Loop over
#for i in range(start,end):
#cap.set(cv.CAP_PROP_POS_FRAMES, i)

# Get bounding box coords for target quadrant
if quadrant == 'q1':
    top = 0 
    bottom = round(((hei - 1) / 2) + 10) 
    left = round((wid - 1) / 2)
    right = wid - 1
elif quadrant == 'q2':
    top = 0 
    bottom = round(((hei - 1) / 2) + 10) 
    left = 0
    right = round(((wid - 1) / 2) + 10)
elif quadrant == 'q3':
    top = round(((hei - 1) / 2) + 5) 
    bottom = hei - 1
    left = 0
    right = round(((wid - 1) / 2) + 10)
elif quadrant == 'q4':
    top = round(((hei - 1) / 2) + 5) 
    bottom = hei - 1
    left = round(((wid - 1) / 2) + 5)    
    right = wid  - 1
else:
    print('Invalid quadrant')

# Get size of output video
size = (right - left, bottom - top)

# Define the codec and create VideoWriter object
fourcc = cv.VideoWriter_fourcc('m', 'p', '4', 'v')
out = cv.VideoWriter(out_file, fourcc, fps, size, isColor=True)

# Capture frame-by-frame
i = start
while i in range(start,end):
    cap.set(cv.CAP_PROP_POS_FRAMES, i)
    # Capture frame-by-frame
    ret, frame = cap.read()
    # if frame is read correctly ret is True
    if not ret:
        print("Can't receive frame (stream end?). Exiting ...")
        break
    # Flip if tank side is "R"    
    if tank_side == 'R':
        frame = cv.rotate(frame, cv.ROTATE_180)
    # Crop frame
    frame = frame[top:bottom, left:right]
    # Write frame
    out.write(frame)
    # Show image    
#    cv.imshow('frame', frame)
    # Add to counter
    i += 1
    # Press 'esc' to close video
    if cv.waitKey(1) == 27:
        cv.destroyAllWindows()
        cv.waitKey(1)
        break

cap.release()
out.release()
out = None
cv.destroyAllWindows()
cv.waitKey(1)

```

## Put in script and test

```{bash}
# On local
code/scripts/20201111_split_videos.py \
  --in_file /Users/brettell/Documents/Data/20201019_pilot_dlc_training/20190613_1024_icab_hni_R.avi \
  --start 2889\
  --end 22790\
  --quadrant q1 \
  --out_dir /Users/brettell/Documents/Data/20201019_pilot_dlc_training_processed
  
# On cluster
conda activate pilot_py

pilot_paper/code/scripts/20201111_split_videos.py \
  --in_file /nfs/ftp/private/birney-res-ftp/upload/medaka/videos/ian_pilot/all/20190613_1024_icab_hni_R.avi \
  --start 2889\
  --end 22790\
  --quadrant q1 \
  --out_dir tmp
# Works!  

# Pull to local to test
scp brettell@ebi:/hps/research1/birney/users/ian/pilot/split/novel_object/20190616_1622_icab_ho5_L_q3.mp4 ~/Desktop

# Change names manually
for i in $( find split/novel_object/* ) ; do
  file_pref=$( echo $i | cut -f1 -d'.' ) ;
  mv $i $file_pref\_novel_object.mp4 ;
done  

for i in $( find split/open_field/* ) ; do
  file_pref=$( echo $i | cut -f1 -d'.' ) ;
  mv $i $file_pref\_open_field.mp4 ;
done  
```

# Label frames on local

## Create `DeepLabCut` conda env on cluster

### CPU
```{bash}
git clone https://github.com/AlexEMG/DeepLabCut.git
conda env create -f DeepLabCut/conda-environments/DLC-CPU.yaml
conda activate DLC-CPU
# Install 2.2 for multi-animal tracking
pip install deeplabcut==2.2b8
# export to snmk dir
conda env export -n DLC-CPU -f pilot_paper/snmk/envs/DLC-CPU.yml
# create working directory
mkdir dlc
```

### GPU
```{bash}
conda env create -f DeepLabCut/conda-environments/DLC-GPU.yaml
conda activate DLC-GPU
# Install 2.2 for multi-animal tracking
pip install deeplabcut==2.2b8
# export to snmk dir
conda env export -n DLC-GPU -f pilot_paper/code/snakemake/envs/DLC-GPU.yml
```

## Start DeepLabCut

```{bash}
ipython
```

```{python}
import deeplabcut
#deeplabcut.launch_dlc() to lauch GUI
```

## Create new project

```{python}
# create list of videos
import os

def get_files(dir_name):
    list_of_file = os.listdir(dir_name)
    complete_file_list = list()
    for file in list_of_file:
        complete_path = os.path.join(dir_name, file)
        if os.path.isdir(complete_path):
            complete_file_list = complete_file_list + get_files(complete_path)
        else:
            complete_file_list.append(complete_path)

    return complete_file_list


dir_name = '/hps/research1/birney/users/ian/pilot/split'
files_list = get_files(dir_name)

# Create project
config_path=deeplabcut.create_new_project('pp_proj', \
                                          'ian_brettell', \
                                          files_list, \
                                          working_directory='/hps/research1/birney/users/ian/pilot/dlc', \
                                          copy_videos=False, \
                                          multianimal=True)

```

Project yaml file: `/hps/research1/birney/users/ian/pilot/dlc/pp_proj-ian_brettell-2020-11-17/config.yaml`

## Extract frames

**Note**: Set `numframes2pick: 2`

```{python}
# to get test frames
deeplabcut.extract_frames(config_path, mode='automatic', algo='uniform', userfeedback=False, crop=False)
# when satisfied with cropping margins, extract by kmeans and set `numframes2pick: 2`
deeplabcut.extract_frames(config_path, mode='automatic', algo='kmeans', userfeedback=False, crop=False)
```

Check whether the quadrant splits need adjustment

```{bash}
mkdir tmp/to_send

for i in $( find dlc/pp_proj-ian_brettell-2020-11-17/labeled-data/*[q1,q3]_open_field -type d ) ; do
  target_file=$( find $i/* | head -1 ) ;
  bname=$( basename $target_file ) ; 
  meta=$( echo $i | cut -f4 -d'/' ) ;
  new_basename=$( echo $meta\_$bname ) ;
  cp $target_file tmp/to_send/$new_basename ;
done  

scp brettell@ebi:/hps/research1/birney/users/ian/pilot/tmp/to_send/* ~/Desktop/frames
```

Put the following lines in `tmp/txt`:

20190611_1331_icab_icab_R
20190611_1454_icab_hni_R
20190612_0952_icab_icab_R
20190612_1053_icab_kaga_R
20190612_1236_icab_kaga_R
20190612_1425_icab_kaga_R
20190612_1501_icab_icab_R
20190612_1534_icab_hdr_R
20190613_1511_icab_hdr_L
20190614_1108_icab_ho5_L
20190614_1208_icab_icab_L
20190614_1456_icab_ho5_R
20190615_1051_icab_hdr_L
20190615_1145_icab_hni_R
20190615_1210_icab_ho5_R
20190615_1649_icab_icab_L
20190616_1017_icab_kaga_R
20190616_1325_icab_hni_R
20190616_1506_icab_hni_L

```{bash}
for i in $( cat tmp/tmp.txt ) ; do
  rm split/*/$i* ;
done
```

Then re-run snakefile and repeat. **NOTE** for the next time we do this, make new `split_script.py` that only creates very short videos, and run that until you have the borders fully adjusted.

## Label frames

Pull project folder to local

**NOTE**: it copied all the videos over as well. Figure out how to exclude next time.

```{bash}
scp -r brettell@ebi:/hps/research1/birney/users/ian/pilot/dlc/pp_proj-ian_brettell-2020-11-17 ~/Desktop/
config.yaml
```


```{python}
deeplabcut.label_frames(config_path)
```

## Train the network

Upload project to Google Drive, and change `project_path` in `config.yaml` to the new path `/gdrive/MyDrive/pilot_paper/pp_proj-ian_brettell-2020-11-17`

Notebook for the process is here: `/gdrive/MyDrive/pilot_paper/20201118_dlc.ipynb`

## Analyse all videos using trained model

### Send all directories to EBI cluster

*BAD* way

```{bash}
# remove old directory
rm -rf dlc/pp_proj-ian_brettell-2020-11-17
# copy over whole directory from Google Drive
rclone copy google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-11-17 dlc/pp_proj-ian_brettell-2020-11-17
```

*GOOD* way

```{bash}
rclone copy 
```

## Assemble and refine tracklets

<https://github.com/DeepLabCut/DeepLabCut/blob/master/docs/functionDetails.md#madeeplabcut-critical-point---assemble--refine-tracklets>

Docs require you to test using one of two tracker types: `box` or `skeleton`. 

`box` seems to work much better. Uses a defined slack/margin specified at `boundingboxslack` and connects them across time with a greedy box overlap metric `iou_threshold`.

Those parameters are set out in `<project>/dlc-models/<iteration>/<project-shuffle>/test/inference_cfg.yaml`

## Extract outlier frames

```{python}
config_path = '/Users/brettell/Desktop/pp_proj-ian_brettell-2020-11-17/config.yaml'
target_video = ['/Users/brettell/Desktop/pp_proj-ian_brettell-2020-11-17/videos/20190611_1410_icab_kaga_R_q4_novel_object.mp4']
deeplabcut.extract_outlier_frames(config_path, target_video, outlieralgorithm='jump')
```

*20201203* 

Unfortunately it appears that you can only extract frames after anlayzing the video *and* refining the tracklets. 

Extract frames on local and label, then sync full folder of labelled frames to Google Drive (noting that it only changes the destination).

```{bash}
rclone sync ~/Desktop/pp_proj-ian_brettell-2020-11-17/labeled-data google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-11-17/labeled-data
```

# iteration-2

While iteration-2 is running, pull over the 20 analyzed videos from iteration-1 to work on evaluation, cross-validation, and refining tracklets.

```{bash}
# Pull over videos
rclone sync google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-11-17/videos ~/Desktop/pp_proj-ian_brettell-2020-11-17/videos
# Pull over model
rclone sync google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-11-17/dlc-models ~/Desktop/pp_proj-ian_brettell-2020-11-17/dlc-models
# Pull over training dataset
rclone sync google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-11-17/training-datasets ~/Desktop/pp_proj-ian_brettell-2020-11-17/training-datasets
```

Docs for `evaluate_multianimal_crossvalidate()` <https://github.com/DeepLabCut/DeepLabCut/blob/d83a2b9b894904337943ffe0ea677cb49730c5f4/deeplabcut/pose_estimation_tensorflow/evaluate_multianimal.py#L414>:

"The most important variable (in inferencecfg) to cross-validate is `minimalnumberofconnections`
Pass a reasonable range to optimize (e.g. if you have 5 edges from 1 to 5. If you have 4 bpts and 11 connections from 3 to 9)."

Here, we have 11 body parts and 55 connections, so we'll try with `9` to `50`. It's currently `7`.

Also change `topktoretain` (maximum number of animals one expects to see) to `2`. 

Try with `rpck_test` (percent correct keypoints).

```{python}
config_path='/Users/brettell/Desktop/pp_proj-ian_brettell-2020-11-17/config.yaml'

# evaluate
deeplabcut.evaluate_network(config_path, Shuffles=[1], plotting=False)
# cross-validate
deeplabcut.evaluate_multianimal_crossvalidate(config_path, Shuffles=[1], edgewisecondition=True, leastbpts=2, init_points=20, n_iter=50, target='rpck_test')
```

## Assemble tracklets

`box` is certainly superior to `skeleton`.

# New project

The labels for multiple lines are causing a lot of errors. Change to just `ref` and `test`, and amend collected data accordingly.

```{bash}
cp -r  ~/Desktop/pp_proj-ian_brettell-2020-11-17/labeled-data ~/Desktop/labeled-data

# remove all labeled folders
rm -rf ~/Desktop/labeled-data/*labeled
# remove backups
rm ~/Desktop/labeled-data/*/*backup 
```

## Amend CSVs

```{bash}
for in_file in $( find ~/Desktop/labeled-data/*/CollectedData_ian_brettell.csv ) ; do
  python \
    ~/Documents/Repositories/pilot_paper/code/scripts/20201203_amend_csvs.py \
    $in_file ;
done    
```

## Amend h5s

```{bash}
python ~/Documents/Repositories/pilot_paper/code/scripts/20201203_amend_h5s.py /Users/brettell/Desktop/labeled-data/20190616_1717_icab_icab_L_q4_novel_object/CollectedData_ian_brettell.h5

for in_file in $( find ~/Desktop/labeled-data/*/CollectedData_ian_brettell.h5 ) ; do
  python \
    ~/Documents/Repositories/pilot_paper/code/scripts/20201203_amend_h5s.py \
    $in_file ;
done
```

## Create new project

```{bash}
# Copy videos over to desktop
cp -r ~/Desktop/pp_proj-ian_brettell-2020-11-17/videos ~/Desktop/videos
# Remove non-videos
rm ~/Desktop/videos/*[pickle,h5]
```

Here: `~/Desktop/pp_proj-ian_brettell-2020-12-03`

Problem files: 

labeled-data/20190612_1053_icab_kaga_R_q3_novel_object
labeled-data/20190612_1326_icab_kaga_R_q2_novel_object
labeled-data/20190612_1534_icab_hdr_R_q3_novel_object
labeled-data/20190616_1256_icab_kaga_L_q1_novel_object
labeled-data/20190616_1441_icab_hni_L_q3_novel_object
labeled-data/20190616_1506_icab_hni_L_q4_novel_object
labeled-data/20190616_1717_icab_icab_L_q1_open_field

## Send to Google Drive

```{bash}
rclone copy ~/Desktop/pp_proj-ian_brettell-2020-12-03 google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-12-03
# Then copied over videos
```

Focused training with:

20190612_1707_icab_hni_R_q2_novel_object
20190612_1326_icab_kaga_R_q4_open_field

## Sync analyzed videos back to local

```{bash}
mkdir ~/Desktop/pp_proj-ian_brettell-2020-12-03/analyzed

rclone sync google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-12-03/analyzed ~/Desktop/pp_proj-ian_brettell-2020-12-03/analyzed

rclone sync google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-12-03 ~/Desktop/pp_proj-ian_brettell-2020-12-03
```

## Create tracklets on local without GUI

```{python}
config_path='/Users/brettell/Desktop/pp_proj-ian_brettell-2020-12-03/config.yaml' 

deeplabcut.convert_detections2tracklets(config_path, ['/Users/brettell/Desktop/pp_proj-ian_brettell-2020-12-03/videos/20190614_1139_icab_hni_R_q4_open_field.mp4'], destfolder = '/Users/brettell/Desktop/pp_proj-ian_brettell-2020-12-03/analyzed', videotype='mp4', shuffle=1, trainingsetindex=0, track_method='box')
```

Can't find the `.pickle` file. Move to `/videos` so they're in the same directory.

## `inference_cfg.yaml` parameter optimisation

* `iou_threshold`: starts at `0.2`.
  - Intersection over Union threshold for linking two bounding boxes.
  - increased to `0.8` and it lost the animals earlier, and misplaced the centre.
  - reduced to `0.1` - better
  - reduced to `0` - works fantastically!
* `max_age`: starts at `100`
  - maximum duration of a tracklet and length
  - increased to `9999`
* `min_hits`: starts at `3`
  - reduce to `1`
* `boundingboxslack`: starts at `0`
  - p/m pixels in width and height for increasing bounding boxes
  - increased to `10` - improved it substantially.
  - increased to `20` - pretty good, still a lot of missing
  
  
Videos for training:
* 201911_1628_icab_hdr_R_q3_novel_object

```{bash}
# Sync with cluster
bsub \
  -o log/20201204_sync.out \
  -e log/20201204_sync.err \
  bsub \
    "rclone sync \
      google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-12-03 \
      dlc/pp_proj-ian_brettell-2020-12-03"
```

*20201208*

# Iteration-1

Metadata on status of videos: `pilot_paper/data/videos_to_analyse.csv`

```{r}
# Full list
meta = read.csv(here::here("data", "videos_to_analyse.csv"),
                header = T, na.strings = "")
meta$analysed = ifelse(meta$completed == "yes", "yes", NA)
# Read in current lines
current = scan(here::here("data", "20201208_iteration1_samples.txt"),what = "character")
meta$analysed = ifelse(meta$sample %in% current, "yes", NA)
# Reorder
meta = meta %>% dplyr::select(sample, analysed, completed)

# 
meta %>% 
  dplyr::filter(is.na(completed)) %>% 
  dplyr::select(sample)
  
```


## Sync labels from local

```{bash, eval = F}
rclone sync \
  ~/Desktop/pp_proj-ian_brettell-2020-12-03/labeled-data \
  google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-12-03/labeled-data
```

## After training, send back model and new videos

```{bash, eval = F}
# Pickles
for i in $( cat data/20201208_iteration1_samples.txt ) ; do
  rclone copy \
    --include "$i*.pickle" \
    google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-12-03/videos \
    ~/Desktop/pp_proj-ian_brettell-2020-12-03/videos ;
done
  
# Model
rclone sync \
  google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-12-03/dlc-models \
  ~/Desktop/pp_proj-ian_brettell-2020-12-03/dlc-models

# Config
rclone sync \
  google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-12-03/config.yaml \
  ~/Desktop/pp_proj-ian_brettell-2020-12-03
  
```

### Iteration-2 

20190615_1022_icab_ho5_L_q4_novel_object
20190615_1305_icab_ho5_R_q2_novel_object
20190612_1501_icab_icab_R_q4_novel_object
20190615_1236_icab_hdr_R_q3_novel_object
20190615_1210_icab_ho5_R_q2_novel_object
20190616_1531_icab_hdr_L_q1_open_field
20190615_1145_icab_hni_R_q4_open_field
20190616_1256_icab_kaga_L_q2_novel_object
20190615_1145_icab_hni_R_q2_novel_object
20190616_1227_icab_kaga_R_q4_open_field
20190616_1531_icab_hdr_L_q3_novel_object
20190613_1054_icab_hdr_R_q3_novel_object
20190614_1108_icab_ho5_L_q2_novel_object
20190614_1553_icab_hni_L_q4_novel_object
20190616_1622_icab_ho5_L_q3_novel_object
20190612_1355_icab_ho5_R_q2_novel_object
20190615_1714_icab_icab_L_q4_novel_object
20190612_1123_icab_hni_R_q4_novel_object
20190614_1108_icab_ho5_L_q3_novel_object
20190614_1644_icab_hni_L_q4_open_field
20190612_1604_icab_hni_R_q1_novel_object
20190614_1335_icab_ho5_L_q4_open_field
20190616_1556_icab_hdr_L_q1_novel_object
20190615_1210_icab_ho5_R_q4_novel_object
20190615_1051_icab_hdr_L_q2_novel_object
20190616_1351_icab_ho5_L_q2_open_field
20190615_1236_icab_hdr_R_q3_open_field
20190614_1430_icab_hdr_L_q1_novel_object
20190614_1335_icab_ho5_L_q1_open_field
20190616_1651_icab_icab_L_q1_open_field

Test video to experiment with:

`20190615_1305_icab_ho5_R_q2_novel_object`

In folder in project `analysing/iteration-2`

Differences in `inference_cfg.yaml` cross-validated parameters:
- when `rpck_train` is the parameter:
  - `pafthreshold`: 0.053887747123252255
- when `rpck_test` is the parameter:
  - `pafthreshold`: 0.12506935309304157
- when `pck_test` is the parameter:
  - `pafthreshold`: 0.12506935309304157   
- when `rmse_test` is the parameter:
  - `pafthreshold`: 0.4007223082503693

Comparing the tracklets, `rpck_test` and `rmse_test` look pretty similar, although for some reason `ref` is correctly labelled in `rpck_test`.

`min_hits`: Higher spreads out the points more during errors.
`boundingboxslack`: Didn't seem to make a difference between 0 and 20
`max_age`: 9999 is better than 0 - the latter loses tracks

So best parameters seem to be:

`boundingboxslack`: 0
`iou_threshold`: 0
`max_age`: 9999
`min_hits`: 0


Bring over pickle:
```{bash}
rclone copy \
  --include "20190615_1305_icab_ho5_R_q2_novel_objectDLC_resnet50_pp_projDec3shuffle1_60000*pickle" \
  google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-12-03/videos \
  ~/Desktop/pp_proj-ian_brettell-2020-12-03/analysing/iteration-2              
```



Setting `max gap fill` to 0 creates spikes in the plots that make it very easy to identify the crossovers.

## iteration-3

### Refinement samples

* 20190615_1452_icab_hdr_L_q4_novel_object.mp4
* 20190615_1210_icab_ho5_R_q1_novel_object.mp4
* 20190615_1452_icab_hdr_L_q2_novel_object.mp4
* 20190611_1331_icab_icab_R_q2_novel_object.mp4
* 20190616_1622_icab_ho5_L_q3_open_field.mp4

### Sync labels from local

```{bash, eval = F}
rclone sync \
  ~/Desktop/pp_proj-ian_brettell-2020-12-03/labeled-data \
  google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-12-03/labeled-data
```

### Sync models grom drive to cluster

```{bash}
rclone sync \
  google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-12-03 \
  dlc/pp_proj-ian_brettell-2020-12-03
```


### Convert raw tracklets to h5 to extract outliers

```{python}
# load config file
config_path = '/Users/brettell/Desktop/pp_proj-ian_brettell-2020-12-03/config.yaml'
# set videofile_path
videofile_path = ['/Users/brettell/Desktop/pp_proj-ian_brettell-2020-12-03/videos/20190613_1054_icab_hdr_R_q3_novel_object.mp4']
# convert to tracklets
deeplabcut.convert_detections2tracklets(config_path, videofile_path, videotype='mp4', shuffle=1, trainingsetindex=0, track_method='box')
# get picklefile
picklefile = []
for file in videofile_path:
  stripped_file = file.strip('.mp4')
  picklefile.append(stripped_file + 'DLC_resnet50_pp_projDec3shuffle1_60000_bx.pickle')

# convert raw tracks to h5
for file in picklefile:
  deeplabcut.convert_raw_tracks_to_h5(config_path, file)
# extract outliers
deeplabcut.extract_outlier_frames(config_path, videofile_path, outlieralgorithm='jump')
deeplabcut.extract_outlier_frames(config_path, videofile_path, outlieralgorithm='uncertain')
deeplabcut.extract_outlier_frames(config_path, videofile_path, outlieralgorithm='fitting')
# refine labels
```

### Copy over videos and analysis files

```{bash}
mkdir ~/Desktop/pp_proj-ian_brettell-2020-12-03/analysing/iteration-3

for i in $(cat ~/Documents/Repositories/pilot_paper/data/20201211_iteration-3_samples.txt ) ; do
  rclone copy \
    --include "$i*" \
    google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-12-03/videos \
    ~/Desktop/pp_proj-ian_brettell-2020-12-03/analysing/iteration-3 ;
done

# and config
rclone copy \
  google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-12-03/config.yaml \
  ~/Desktop/pp_proj-ian_brettell-2020-12-03
  
# and model
rclone sync \
  google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-12-03/dlc-models \
  ~/Desktop/pp_proj-ian_brettell-2020-12-03/dlc-models
```

### Create h5
```{python}
# load config file
config_path = '/Users/brettell/Desktop/pp_proj-ian_brettell-2020-12-03/config.yaml'
# set videofile_path
sample_file_path = '/Users/brettell/Documents/Repositories/pilot_paper/data/20201211_iteration-3_samples.txt'
analysis_path = '/Users/brettell/Desktop/pp_proj-ian_brettell-2020-12-03/analysing/iteration-3'
# read in samples
with open(sample_file_path, 'r') as f:
  videofile_path = f.readlines()

videofile_path = [x.strip() + '.mp4'  for x in videofile_path]  

# add pickle file suffix
pickle_suff = 'DLC_resnet50_pp_projDec3shuffle1_60000_bx.pickle'
pickle_paths = [os.path.join(analysis_path, x.strip('.mp4') + pickle_suff) for x in videofile_path]  

# convert raw tracks to h5
for file in pickle_paths:
  deeplabcut.convert_raw_tracks_to_h5(config_path, file)
# extract outliers
deeplabcut.extract_outlier_frames(config_path, videofile_path, outlieralgorithm='jump')
deeplabcut.extract_outlier_frames(config_path, videofile_path, outlieralgorithm='uncertain')
deeplabcut.extract_outlier_frames(config_path, videofile_path, outlieralgorithm='fitting')
# refine labels
```

## iteration-6

`inference_cgf.yaml` parameters:

`boundingboxslack` doesn't seem to make a difference.

### Copy over videos

```{bash}
i=6

for j in $(cat ~/Documents/Repositories/pilot_paper/data/20201213_iteration-$i\_samples.txt ) ; do
  rclone copy \
    --include "$j*" \
    google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-12-03/videos \
    ~/Desktop/pp_proj-ian_brettell-2020-12-03/analysing/iteration-$i ;
done

# and config
rclone copy \
  google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-12-03/config.yaml \
  ~/Desktop/pp_proj-ian_brettell-2020-12-03
  
# and model
rclone sync \
  google_drive_ebi:pilot_paper/pp_proj-ian_brettell-2020-12-03/dlc-models/iteration-$i \
  ~/Desktop/pp_proj-ian_brettell-2020-12-03/dlc-models/iteration-$i
```

### Create h5
```{python}
import os
# load config file
config_path = '/Users/brettell/Desktop/pp_proj-ian_brettell-2020-12-03/config.yaml'
# set iteration
iteration = '6'
# set videofile_path
sample_file_path = '/Users/brettell/Documents/Repositories/pilot_paper/data/20201213_iteration-' + iteration + '_samples.txt'
analysis_path = '/Users/brettell/Desktop/pp_proj-ian_brettell-2020-12-03/analysing/iteration-' + iteration
# read in samples
with open(sample_file_path, 'r') as f:
  videofile_path = f.readlines()

videofile_path = [x.strip() + '.mp4'  for x in videofile_path]  

# add pickle file suffix
pickle_suff = 'DLC_resnet50_pp_projDec3shuffle1_50000_bx.pickle'
pickle_paths = [os.path.join(analysis_path, x.strip('.mp4') + pickle_suff) for x in videofile_path]  

# convert raw tracks to h5
for file in pickle_paths:
  deeplabcut.convert_raw_tracks_to_h5(config_path, file)
# extract outliers
deeplabcut.extract_outlier_frames(config_path, videofile_path, outlieralgorithm='jump')
deeplabcut.extract_outlier_frames(config_path, videofile_path, outlieralgorithm='uncertain')
deeplabcut.extract_outlier_frames(config_path, videofile_path, outlieralgorithm='fitting')
# refine labels
```